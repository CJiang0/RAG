# RAG System for Existing Knowledge Base

A production-ready Retrieval-Augmented Generation (RAG) system that provides intelligent question-answering capabilities over a company knowledge base using advanced chunking strategies and comprehensive evaluation metrics.

## ğŸŒŸ Features

- **Intelligent Document Processing**: Automatically splits documents into semantically meaningful chunks with headlines and summaries using LLM-powered preprocessing
- **Advanced Retrieval**: Two-stage retrieval pipeline with semantic search and LLM-based reranking
- **Comprehensive Evaluation**: Built-in evaluation framework with MRR, nDCG, and LLM-as-a-judge metrics
- **Multi-domain Knowledge Base**: Handles company info, employee data, product details, and contracts
- **Parallel Processing**: Multi-threaded document ingestion for faster preprocessing
- **Robust Error Handling**: Automatic retries with exponential backoff for API calls

## ğŸ“ Project Structure

```
RAG_github/
â”œâ”€â”€ ingest.py                   # Document preprocessing and vectorization
â”œâ”€â”€ evaluation_func/
â”‚   â”œâ”€â”€ answer.py              # RAG query pipeline with reranking
â”‚   â”œâ”€â”€ eval.py                # Evaluation metrics (MRR, nDCG, LLM judge)
â”‚   â”œâ”€â”€ test.py                # Test case utilities
â”‚   â””â”€â”€ tests.jsonl            # Evaluation test dataset
â”œâ”€â”€ evaluation.ipynb           # Jupyter notebook for running evaluations
â”œâ”€â”€ knowledge-base/            # Source documents
â”‚   â”œâ”€â”€ company/              # Company information
â”‚   â”œâ”€â”€ contracts/            # Contract documents
â”‚   â”œâ”€â”€ employees/            # Employee profiles
â”‚   â””â”€â”€ products/             # Product documentation
â”œâ”€â”€ preprocessed_db/          # ChromaDB vector database (generated)
â””â”€â”€ pyproject.toml            # Project dependencies

```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.11+
- [uv](https://github.com/astral-sh/uv) package manager
- OpenAI API key

### Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/RAG_github.git
cd RAG_github
```

2. Install dependencies:
```bash
uv sync
```

3. Set up environment variables:
Create a `.env` file in the project root:
```env
OPENAI_API_KEY=your_openai_api_key_here
```

### Data Ingestion

Process documents and create the vector database:

```bash
uv run ingest.py
```

This will:
1. Load all markdown documents from `knowledge-base/`
2. Split documents into intelligent chunks using GPT-4.1-nano
3. Generate headlines and summaries for each chunk
4. Create embeddings using `text-embedding-3-large`
5. Store everything in ChromaDB

**Note**: Adjust `WORKERS` in `ingest.py` if you encounter rate limits (default: 3).

## ğŸ’¬ Usage

### Command-Line Interface

Run a single evaluation test:

```bash
uv run evaluation_func/eval.py <test_number>
```

Example:
```bash
uv run evaluation_func/eval.py 0
```

This will display:
- Question and reference answer
- Retrieval metrics (MRR, nDCG, keyword coverage)
- Generated answer with LLM judge scores

### Programmatic Usage

```python
from evaluation_func.answer import answer_question

# Ask a question
answer, context_docs = answer_question("What products does Insurellm offer?")
print(answer)
```

### Jupyter Notebook

For comprehensive evaluation, use the provided notebook:

```bash
jupyter notebook evaluation.ipynb
```

The notebook provides:
- Batch retrieval evaluation across all test cases
- LLM-as-a-judge answer quality assessment
- Category-wise performance analysis
- Summary statistics and visualizations

## ğŸ“Š Evaluation Metrics

### Retrieval Metrics

- **MRR (Mean Reciprocal Rank)**: Measures how quickly relevant documents are retrieved
- **nDCG (Normalized Discounted Cumulative Gain)**: Evaluates ranking quality
- **Keyword Coverage**: Percentage of expected keywords found in retrieved context

### Answer Quality Metrics (LLM-as-a-Judge)

- **Accuracy** (1-5): Factual correctness compared to reference answer
- **Completeness** (1-5): Coverage of all aspects in the reference answer
- **Relevance** (1-5): Direct addressing of the question without extra information

## ğŸ”§ Configuration

### Key Parameters in `ingest.py`

```python
MODEL = "openai/gpt-4.1-nano"          # LLM for chunk generation
embedding_model = "text-embedding-3-large"  # Embedding model
AVERAGE_CHUNK_SIZE = 100               # Target words per chunk
WORKERS = 3                            # Parallel workers
```

### Key Parameters in `answer.py`

```python
MODEL = "openai/gpt-4.1-nano"          # LLM for reranking & answering
RETRIEVAL_K = 20                       # Initial retrieval count
FINAL_K = 10                           # Final reranked chunks
```

## ğŸ—ï¸ Architecture

### 1. Document Ingestion Pipeline

```
Documents â†’ LLM Chunking â†’ Headline/Summary Generation â†’ Embeddings â†’ ChromaDB
```

Each chunk contains:
- **Headline**: Brief query-optimized heading
- **Summary**: Condensed content for quick understanding
- **Original Text**: Complete original content

### 2. Query Pipeline

```
Query â†’ Embedding â†’ Retrieve Top-K â†’ LLM Reranking â†’ Generate Answer
```

Two-stage retrieval ensures both recall and precision.

## ğŸ§ª Testing

The project includes a comprehensive test suite in `tests.jsonl` covering:
- Direct factual questions
- Multi-document queries
- Category-specific questions (products, employees, contracts, etc.)

Run all evaluations:

```python
# In evaluation.ipynb
for test, result, progress in evaluate_all_retrieval():
    print(f"Test {progress*100:.0f}% complete")
```

## ğŸ“ Adding New Documents

1. Add markdown files to appropriate `knowledge-base/` subfolder
2. Re-run ingestion:
```bash
uv run ingest.py
```

The system will automatically process new documents.

