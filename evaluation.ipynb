{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90988ebd",
   "metadata": {},
   "source": [
    "# RAG evaluation \n",
    "What it does:\n",
    "- Runs all retrieval tests and summarizes MRR, nDCG, and keyword coverage.\n",
    "- Groups retrieval quality by category.\n",
    "- Runs LLM-as-judge answer evaluation (may incur model cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22debe47",
   "metadata": {},
   "source": [
    "## Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdac6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evaluation_func.eval import evaluate_all_retrieval, evaluate_all_answers\n",
    "retrieval_rows = []\n",
    "for idx, (test, result, progress) in enumerate(evaluate_all_retrieval(), 1):\n",
    "    retrieval_rows.append({\n",
    "        \"test_index\": idx - 1,\n",
    "        \"category\": test.category,\n",
    "        \"question\": test.question,\n",
    "        \"mrr\": result.mrr,\n",
    "        \"ndcg\": result.ndcg,\n",
    "        \"coverage\": result.keyword_coverage,\n",
    "        \"keywords_found\": result.keywords_found,\n",
    "        \"total_keywords\": result.total_keywords,\n",
    "        \"progress\": progress,\n",
    "    })\n",
    "\n",
    "retrieval_df = pd.DataFrame(retrieval_rows)\n",
    "print(f\"Completed retrieval tests: {len(retrieval_df)}\")\n",
    "\n",
    "retrieval_summary = pd.DataFrame({\n",
    "    \"mrr_mean\": [retrieval_df['mrr'].mean()],\n",
    "    \"ndcg_mean\": [retrieval_df['ndcg'].mean()],\n",
    "    \"coverage_mean_pct\": [retrieval_df['coverage'].mean()],\n",
    "})\n",
    "\n",
    "category_mrr = (\n",
    "    retrieval_df.groupby('category')['mrr']\n",
    "    .mean()\n",
    "    .reset_index(name='avg_mrr')\n",
    "    .sort_values('avg_mrr', ascending=False)\n",
    ")\n",
    "\n",
    "display(retrieval_summary)\n",
    "display(category_mrr)\n",
    "display(retrieval_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb05bd5",
   "metadata": {},
   "source": [
    "## Answer Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0657e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle answer evaluation (LLM judge). Set to False to avoid model calls/cost.\n",
    "RUN_ANSWER_EVAL = True\n",
    "if RUN_ANSWER_EVAL:\n",
    "    answer_rows = []\n",
    "    for idx, (test, result, progress) in enumerate(evaluate_all_answers(), 1):\n",
    "        answer_rows.append({\n",
    "            \"test_index\": idx - 1,\n",
    "            \"category\": test.category,\n",
    "            \"question\": test.question,\n",
    "            \"accuracy\": result.accuracy,\n",
    "            \"completeness\": result.completeness,\n",
    "            \"relevance\": result.relevance,\n",
    "            \"feedback\": result.feedback,\n",
    "            \"progress\": progress,\n",
    "        })\n",
    "\n",
    "    answer_df = pd.DataFrame(answer_rows)\n",
    "    print(f\"Completed answer tests: {len(answer_df)}\")\n",
    "\n",
    "    answer_summary = pd.DataFrame({\n",
    "        \"accuracy_mean\": [answer_df['accuracy'].mean()],\n",
    "        \"completeness_mean\": [answer_df['completeness'].mean()],\n",
    "        \"relevance_mean\": [answer_df['relevance'].mean()],\n",
    "    })\n",
    "\n",
    "    category_accuracy = (\n",
    "        answer_df.groupby('category')['accuracy']\n",
    "        .mean()\n",
    "        .reset_index(name='avg_accuracy')\n",
    "        .sort_values('avg_accuracy', ascending=False)\n",
    "    )\n",
    "\n",
    "    display(answer_summary)\n",
    "    display(category_accuracy)\n",
    "    display(answer_df.head())\n",
    "else:\n",
    "    print(\"Skipping answer evaluation; set RUN_ANSWER_EVAL = True to run.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
